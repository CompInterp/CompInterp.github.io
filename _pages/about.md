---
layout: about
title: about
permalink: /
subtitle:

profile:
  align: right
  image: logo_dark.svg
  image_circular: false # crops the image to make it circular
  more_info:
    # Our latest <a href="https://www.youtube.com/watch?v=yUGZVPJlvzY">hit single</a>
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

**CompInterp** uncovers the structure of neural representations, showing how simple features combine into complex behaviours.
By unifying tensor and neural network paradigms, model weights and data are treated as a single modality. This compositional lens on design, analysis and control paves the way for inherently interpretable AI without compromising performance.

Compositional architectures capture rich non-linear (polynomial) relationships between representation spaces. Instead of masking them through linear approximations, CompInterp methods expose their inherent hierarchical structure accross levels of abstraction.
This allows for **weight-based** subcircuit analysis, grounding interpretability in formal (de)compositions rather than post-hoc activation-based heuristics.

We're now scaling compositional interpretability to transformers and CNNs by leveraging their low-rank structure through tensor decomposition and information theory.
Learn more in our [latest talk](https://www.youtube.com/watch?v=yUGZVPJlvzY)!

<!-- The **CompInterp** approach to interpretability treats weights and data as a unified modality to provide a compositional perspective on model design, analysis, and manipulation. By combining tensor and neural network paradigms, our $\chi$-nets pave the way for inherently interpretable AI without sacrificing performance.

**$\chi$-nets** are compositional by design, both in how they are built and in the representations they learn. Their architecture enables mathematical guarantees and weight-based subcircuit analysis, grounding interpretability in formal (de)compositions rather than post-hoc activation-based approximations.

We're currently scaling CompInterp methods to CNNs and transformers by leveraging their specialised low-rank structure. Learn more about it in our [latest talk](https://www.youtube.com/watch?v=yUGZVPJlvzY)! -->

<!-- Compositional interpretability aims to uncover the structure of neural representations, revealing how simple features compose into sophisticated behaviours. Combining tensor and neural network paradigms paves the way for inherently interpretable AI without sacrificing performance. This approach treats weights and data as a unified modality and provides a formal perspective on model analysis, steering and design.

Compositional architectures enable weight-based subcircuit analysis and grounding interpretability in formal (de)compositions rather than post-hoc activation-based approximations. This allows flexible analysis at any granularity. Compositional models can be described using (polynomial) non-linearities, embracing the inherent hierarchies in models rather than obscuring them through linear approximations.

We're currently scaling compositional interpretability to transformers and CNNs by leveraging modern advances in tensor networks and information theory. Learn more about it in our [latest talk](https://www.youtube.com/watch?v=yUGZVPJlvzY)! -->

