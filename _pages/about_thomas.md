## **Thomas** Dooms

Hey! I'm a PhD student exploring weight-based interpretability.
I'm interested in understanding how neural networks encode complex behaviours and knowledge.

Modern deep networks are astonishingly intricate and difficult to disentangle.
I argue that we should embrace this complexity rather than shoehorn them into simplistic feature sets, flattening the very structures we hope to understand.

My work treats a model's weights as a compositional system whose parts interact and recombine, much like phrases in a language.
By developing flexible, formally grounded tools for compositional analysis, I aim to reveal how rich mechanisms and representations arise and how we can reason about them.
